{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/CRTS2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import astropy.time as astime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../..\")\n",
    "import measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import transient lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451474, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'transient_lightcurves.pickle'\n",
    "indir = DATA_PATH; filepath = indir + filename\n",
    "df_tra = pd.read_pickle(filepath)\n",
    "df_tra.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter transient lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete rows of blended observations\n",
    "df_tra = df_tra.drop_duplicates(['TransientID','MJD'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add observation count to every transient\n",
    "df_count = df_tra.groupby('TransientID', as_index=False).count()\n",
    "df_count['ObsCount'] = df_count['Mag']\n",
    "df_count = df_count[['TransientID', 'ObsCount']]\n",
    "df_tra = df_tra.merge(df_count, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove objects with less than 5 observations\n",
    "df_tra = df_tra[df_tra.ObsCount >= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import permanent lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1924409, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'permanent_lightcurves.pickle'\n",
    "indir = DATA_PATH; filepath = indir + filename\n",
    "df_per = pd.read_pickle(filepath)\n",
    "df_per.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1802695, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete rows of blended observations\n",
    "df_per = df_per.drop_duplicates(['ID','MJD'], keep='first')\n",
    "df_per.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add observation count to every permanent\n",
    "df_count = df_per.groupby('ID', as_index=False).count()\n",
    "df_count['ObsCount'] = df_count['Mag']\n",
    "df_count = df_count[['ID', 'ObsCount']]\n",
    "df_per = df_per.merge(df_count, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1798465, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove objects with less than 5 observations\n",
    "df_per = df_per[df_per.ObsCount >= 5]\n",
    "df_per.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15193,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_per.ID.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512281, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample subset of same size as transients\n",
    "sample_size = df_tra.TransientID.unique().shape[0]\n",
    "IDs = np.random.choice(df_per.ID.unique(), size=sample_size, replace=False)\n",
    "df_per = df_per[df_per.ID.isin(IDs)]\n",
    "df_per.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define extract features functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stetson_k(data):\n",
    "  '''\n",
    "  Welch-Stetson variability index K (Stetson 1996)\n",
    "  '''\n",
    "  n = len(data[1])\n",
    "  stetson_k = 0\n",
    "  if n > 1:\n",
    "    if len(data) > 2:\n",
    "      delta = np.sqrt(float(n) / (n - 1)) * (data[1] - np.mean(data[1])) / data[2] \n",
    "    else:\n",
    "      delta = np.sqrt(float(n) / (n - 1)) * (data[1] - np.mean(data[1]))\n",
    "    top = abs(delta).sum() / n\n",
    "    bottom = np.sqrt((delta * delta).sum() / n)\n",
    "    stetson_k = top / bottom\n",
    "  return stetson_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, feature_dict):\n",
    "    df = df.copy()\n",
    "    df['Flux'] = measurements.__mag_to_flux__(df.Mag)\n",
    "    df['Date'] = astime.Time(df.MJD, format='mjd').datetime\n",
    "    df = df.sort_values('Date')\n",
    "#    print(df.Mag.as_matrix(), df.Magerr.as_matrix())\n",
    "    data = df[['Date', 'Mag', 'Magerr']].transpose().as_matrix()\n",
    "    feature_dict['stetson_j'].append(stetson_j(data))\n",
    "#    feature_dict['2stetson_k'].append(stetson_k(data))\n",
    "    feature_dict['stetson_j_my'].append(my_stetson_j(df.Mag, df.Magerr, df.Date, True))\n",
    "#    feature_dict['stetson_k'].append(measurements.stetson_k(df.Mag, df.Magerr))\n",
    "    #feature_dict['stetson_k'].append(measurements.stetson_k(df.Mag, df.Magerr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __stetson_sigmas__(ss_mag, ss_magerr):\n",
    "    '''\n",
    "    Calculates the relative errors (sigmas) for stetson measurements.\n",
    "    '''\n",
    "    n = ss_mag.shape[0]\n",
    "    sigmas = np.sqrt(float(n) / (n - 1)) * (ss_mag - ss_mag.mean()) / ss_magerr\n",
    "    return sigmas\n",
    "\n",
    "def __datetime_diff_to_int_timedelta__(ss_datetime_diff):\n",
    "    '''\n",
    "    Convert datetime series to integer timedelta\n",
    "    '''\n",
    "    return ss_datetime_diff.dt.total_seconds() / 3600\n",
    "def __datetime_diff_to_int_timedelta2__(diff):\n",
    "    '''\n",
    "    Convert datetime series to integer timedelta\n",
    "    '''\n",
    "    return diff.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def stetson_j(data):\n",
    "    '''\n",
    "    Welch-Stetson variability index J (Stetson 1996) with weighting scheme from (Zhang et al. 2003)\n",
    "    taking successive pairs in time-order\n",
    "    '''\n",
    "    n = len(data[1])\n",
    "    stetson_j = 0\n",
    "    if n > 1:\n",
    "        if len(data) > 2: \n",
    "            delta = np.sqrt(float(n) / (n - 1)) * (data[1] - np.mean(data[1])) / data[2] \n",
    "        else:\n",
    "            delta = np.sqrt(float(n) / (n - 1)) * (data[1] - np.mean(data[1])) \n",
    "        sum = 0.\n",
    "        w_sum = 0.\n",
    "        dt = 0.\n",
    "        my_sum = 0.\n",
    "        for i in range(n - 1):\n",
    "            dt += ( (data[0][i + 1] - data[0][i]).total_seconds() / 3600 )\n",
    "            my_sum += ((data[0][i + 1] - data[0][i]).total_seconds() / 3600 )\n",
    "            dt = dt / float(n - 1)\n",
    "            if i == n-2: print(dt)\n",
    "        print('MY_SUM', my_sum)\n",
    "        for i in range(n - 1):\n",
    "            wk = numpy.exp(-(data[0][i + 1] - data[0][i]).total_seconds() / 3600 / dt)\n",
    "            #if(i == 0): print(wk)\n",
    "            pk = delta[i] * delta[i + 1]\n",
    "            sum += wk * np.sign(pk) * np.sqrt(abs(pk))\n",
    "            w_sum += wk\n",
    "            stetson_j = sum / w_sum\n",
    "    return stetson_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_stetson_j(ss_mag, ss_magerr, ss_date, exponential=False):\n",
    "    '''\n",
    "    The Welch-Stetson J variability index (Stetson 1996).\n",
    "    A robust standard deviation.\n",
    "    Optional exponential weighting scheme (Zhang et al. 2003) taking successive pairs in time order.\n",
    "    NOTE: ss_flux must be ordered by it's corresponding date in ss_date.\n",
    "    '''\n",
    "    n = ss_mag.shape[0]\n",
    "    if n <= 1: return 0\n",
    "    # Calculate sigmas: Relative Errors\n",
    "    sigmas = __stetson_sigmas__(ss_mag, ss_magerr)\n",
    "    # Calculate weights\n",
    "    w = np.ones(int(n)); w[0] = 0\n",
    "    if exponential:\n",
    "        # Calculate mean dt: delta-time\n",
    "        dt = __datetime_diff_to_int_timedelta__(ss_date.diff()).sum() / (n-1)\n",
    "        print('DT2', __datetime_diff_to_int_timedelta__(ss_date.diff()).sum(), dt)\n",
    "        # Re-calculate Weights\n",
    "        w = np.exp(-__datetime_diff_to_int_timedelta__(ss_date.diff()) / dt)\n",
    "#    print(dt)\n",
    "    print(__datetime_diff_to_int_timedelta__(ss_date.diff()).shape)\n",
    "    # Calculate p: product of residuals\n",
    "    p = sigmas * sigmas.shift(1)\n",
    "    # Return Stetson J measuerement\n",
    "    return (w * np.sign(p) * p.abs().pow(1./2)).sum() / w.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract transient features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488.95619340727063\n",
      "MY_SUM 66581.0811852839\n",
      "DT2 66581.0811853 5548.42343211\n",
      "(13,)\n",
      "{'stetson_j': [-0.76384204983677106], 'stetson_j_my': [-0.7675000063263111]}\n"
     ]
    }
   ],
   "source": [
    "LOCATION = 11\n",
    "feature_dict = {'stetson_j_my': [], 'stetson_j':[]}\n",
    "               # 'stetson_k': [], '2stetson_k':[]}\n",
    "for trID in df_tra.TransientID.unique():\n",
    "    df = df_tra[df_tra.TransientID == trID]\n",
    "#    feature_dict['ID'].append(trID)\n",
    "    extract_features(df, feature_dict)\n",
    "    print(feature_dict)\n",
    "    break\n",
    "#df_feat_tran = pd.DataFrame(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
